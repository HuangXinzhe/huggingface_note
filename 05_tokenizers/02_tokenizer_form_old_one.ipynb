{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7378a23d-8de7-4606-a103-99fbef27a8c9",
   "metadata": {},
   "source": [
    "- 需要使用的语言没有可用的语言模型\n",
    "- 语料库与您的语言模型所使用的语言模型非常不同  \n",
    "需要合适的tokenizer从头开始训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303b041-82a5-4fcd-878e-4fa316eb5685",
   "metadata": {},
   "source": [
    "## 1、建立语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952ab31-4cea-46d6-9cae-9b62e9026200",
   "metadata": {},
   "source": [
    "### 此数据库作者有改动，暂时无法下载\n",
    "    https://huggingface.co/datasets/code_search_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30ad0f-5a92-40a7-89db-3f7c4c0b48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988abc7c-29bc-478e-995f-1a5cb2fc689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649a221-12d5-4d9b-b53f-2148eadf3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset({\n",
    "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', \n",
    "      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', \n",
    "      'func_code_url'\n",
    "    ],\n",
    "    num_rows: 412178\n",
    "})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5b071-65c2-4cfb-9dd3-d317e8538893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4f776-254a-460c-b039-8175220b9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def handle_simple_responses(\n",
    "      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n",
    "    \"\"\"Accepts normal responses from the device.\n",
    "\n",
    "    Args:\n",
    "      timeout_ms: Timeout in milliseconds to wait for each response.\n",
    "      info_cb: Optional callback for text sent from the bootloader.\n",
    "\n",
    "    Returns:\n",
    "      OKAY packet's message.\n",
    "    \"\"\"\n",
    "    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145f560-2468-4a4d-a941-9482cb69270c",
   "metadata": {},
   "source": [
    "### 使用whole_func_string列训练数据，将数据集转换为文本列表的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124aa68-3a53-4f56-9141-5db16bc80361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't uncomment the following line unless your dataset is small!\n",
    "# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600cf1c2-c45e-4696-ac81-f7baad70c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf98a92-120f-478c-a723-e3e879137f03",
   "metadata": {},
   "source": [
    "### 生成器对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46e8f70-3464-4aae-a18c-e7e1fadfe820",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = (i for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd2b032-90d0-473c-bb95-d8693e4648c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ef5d33-83c5-43a1-943f-46449d3ed7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9cd07-925c-4e3b-854d-bcd2d45b19f4",
   "metadata": {},
   "source": [
    "### 建立数据的生成器对象，每次只取一部分数据，即使处理一个巨大的数据集，也不会耗尽所有内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3a7ad-0593-4f13-b013-582c0acbdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c321e3-ddc0-4c8a-9770-1f701e7be989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d852e-c30f-418e-93b3-4866f3f645f2",
   "metadata": {},
   "source": [
    "## 2、训练一个新的tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0a8eb-a59f-4206-963b-93654c3d7be8",
   "metadata": {},
   "source": [
    "### 加载一个tokenizer，与模型配对的\n",
    "    即使从新训练一个tokenizer，也不需要从头开始，不必指定任何关于标记化算法或想要使用的特殊标记的内容; 新tokenizer将与GPT-2完全相同，唯一需要改变的是词汇表，这将由我们的语料库中的训练决定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f428c-e690-40a7-9d02-7ffab805e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b055b-f0a7-4c00-bcca-07ceace7a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf11675-53db-4cd3-a4d0-379f4843d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo',\n",
    " 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924cabb-805a-4f9c-b4b6-0cac38358b84",
   "metadata": {},
   "source": [
    "### 上方是没有训练过的tokenizer，有一些特殊的标记符号，对于_也不能有好的分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fe50a-b325-4744-be5e-bafc75794d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9d2b5-7dfd-4626-84d2-1b4d786fb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301e1a2-1da3-45b5-a4b6-41c778ae4a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`',\n",
    " 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3e611-57ff-4f60-b66d-f6450f19ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c31a5-19bf-4ff1-ab95-a3f1d6af4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "27\n",
    "36\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877eee0-0cd2-4359-b957-4fe36c1cb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672efd6b-e390-4d98-8c28-d58d91961b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',',\n",
    " 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_',\n",
    " 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(',\n",
    " 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ',\n",
    " 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2f840-7876-429b-89d0-cdfcd99ad68e",
   "metadata": {},
   "source": [
    "## 3、保存tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12880178-952f-4ea2-b169-b9d56dfc1cd5",
   "metadata": {},
   "source": [
    "### tokenizer需要重新加载的所有文件都在文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8a477-caff-49a9-a404-2f4e3adae8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43b98d-8628-4403-9518-eb7053b62fec",
   "metadata": {},
   "source": [
    "### 上传Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7bf17-af7c-47a7-8654-f5dc3c69f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 登陆\n",
    "# 终端：huggingface-cli login\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a74bec-0d4c-4a8c-8a5a-f216e78ce9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在命名空间中创建一个名为 code-search-net-tokenizer 的新存储库，其中包含tokenizer文件\n",
    "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a12101-46d7-4211-8094-5046b6b0fb87",
   "metadata": {},
   "source": [
    "### 使用from_pretraining()方法从任何地方加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b312c78-fbc9-4500-9174-708e0be9121c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a291f-f44c-412b-9378-0b68610d3ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0523c4-90b7-4cd3-87dd-0a3609f50ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa7c33-270d-46e4-8e64-e981ec28d42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
