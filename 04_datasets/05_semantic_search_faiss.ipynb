{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e7edc4-b87f-4b92-bf6e-38f5474cf6be",
   "metadata": {},
   "source": [
    "# 基于 FAISS 的语义搜索\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951930a-4378-4bde-afc8-2ddbd245f35d",
   "metadata": {},
   "source": [
    "## 使用嵌入进行语义搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fda773-8a83-49e0-b892-724cd4bf4da0",
   "metadata": {},
   "source": [
    "## 1、加载与准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4857f772-90a8-445b-9ad8-645a839043c0",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869d9f56-9d56-4dff-957f-e5c0e011128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lewtun--github-issues-e63b3881049a19d2\n",
      "Reusing dataset json (/Users/huangxinzhe/.cache/huggingface/datasets/json/lewtun--github-issues-e63b3881049a19d2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9bbc4-2dd6-4b3e-a22f-f767f3458bcc",
   "metadata": {},
   "source": [
    "### 准备数据\n",
    "    过滤pull请求和没有描述的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d702eea2-b820-44d8-a08b-f935c788b720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x125b25940> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007162809371948242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9c1951b98b4a1284adfc7befac3d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635651e-43b2-422c-96d4-00e163b30ecb",
   "metadata": {},
   "source": [
    "从搜索的角度来看，信息量最大的列是 title、 body 和 comments，而 html _ url 为我们提供了回到源问题的链接。让我们使用 Dataset.move _ column ()函数删除剩下的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095fdf8c-b821-4f42-bcb2-98525f539b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)  # 移除两个中重复的元素\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22146827-3896-4e27-8012-48b9a982fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205c6e8a-2b8a-4139-b96c-05c9417632bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0].tolist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8442099-1847-4aef-a1ac-eaa73ad5af8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将comments中每一句话都做成一行数据\n",
    "comments_df = df.explode(\"comments\", ignore_index=True)  \n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342547c4-fcd6-4ed8-b30d-606e66c2caf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987b830-9bbb-4559-bd36-8d7917986656",
   "metadata": {},
   "source": [
    "# 此处2964条数据是因为将comments中的每句话都做成了一条数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc15ba8-48bb-444a-8eac-6180dffb8ad3",
   "metadata": {},
   "source": [
    "### 创建comments_length列，其中包含每条注释的单词数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdc2b58-67ab-401b-a18f-583ba4569d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006788969039916992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2964,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9dcf9c1f3d4608997387220bca1610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2964 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe811e-9e9e-488a-9925-4efe6d16da3f",
   "metadata": {},
   "source": [
    "### 将数据中所有字段数据连接在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c4fcb1b-1b26-4e85-b78e-71a5b0090dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00996088981628418,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2964,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5e49e64cc044e195853c516331e8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2964 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047104f-fef8-4a7f-b3d1-dd450ccab125",
   "metadata": {},
   "source": [
    "## 2、创建文本embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96acf13a-fa2c-4021-80c3-b2b70b01911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07fd268-b0f7-451c-99e1-3af277703988",
   "metadata": {},
   "source": [
    "### 可以将模型在GPU上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08acce6a-7491-483e-aeac-753c571907e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "203ddcba-5e5d-4e28-9a67-6480b7e860e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):  # 获得[cls]最后一个隐藏层结果\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8f7e330-0221-48a7-a5ef-efd227a1c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得每个数据中对应的[cls]，并将其转入GPU上（可以选择）\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    # encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a7bf71-7c96-4d22-bdfc-f0f1bd00ffc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试获取embedding函数\n",
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b418fa-1ec3-4ddc-b94a-bc3572bd2eca",
   "metadata": {},
   "source": [
    "### 获得所有数据的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca9df1cc-e39d-4232-ab8e-d850cd367327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018477916717529297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2964,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b544c5680d46d58ac2428de5806e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2964 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176475d5-c586-4887-aceb-99ed8e827ecc",
   "metadata": {},
   "source": [
    "## 3、使用FAISS进行有效的最近邻搜素\n",
    "    FAISS (short for Facebook AI Similarity Search)\n",
    "    Facebook 人工智能最近邻搜索的缩写\n",
    "    基本思想是创建一个称为索引的特殊数据结构，它允许查找相似词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bddb733c-f013-4675-b61a-36a84e19454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003665924072265625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c1660bb6164df9a24dc031f0afdb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f98419c-a279-42c9-aac2-e43c7e8534d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99694512-bbf7-4eb8-b0fd-b634b2a7a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得5个最相似的结果\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc3ad28b-d899-4e19-8910-33bb4ac8edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对获取的结果以得分高低进行排序\n",
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7d421a-d475-4771-895c-f30c9d063073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "SCORE: 25.505020141601562\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
      "You can now use them offline\n",
      "```python\n",
      "datasets = load_dataset('text', data_files=data_files)\n",
      "```\n",
      "\n",
      "We'll do a new release soon\n",
      "SCORE: 24.555513381958008\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "SCORE: 24.148990631103516\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "SCORE: 22.894001007080078\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "SCORE: 22.40664291381836\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 循环查看结果\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2dc71f-b387-4fa4-9471-3eb96b5f63f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
