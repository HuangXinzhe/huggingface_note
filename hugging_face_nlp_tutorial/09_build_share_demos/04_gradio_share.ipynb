{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYbb75Kx9cqB"
      },
      "source": [
        "## 与他人分享"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "IPluwjj89fOQ",
        "outputId": "7067552d-6637-49dd-ac36-3e410aededa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://bd52fa8cda82a1c723.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://bd52fa8cda82a1c723.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "title = \"Ask Rick a Question\"\n",
        "description = \"\"\"\n",
        "The bot was trained to answer questions based on Rick and Morty dialogues. Ask Rick anything!\n",
        "<img src=\"https://huggingface.co/spaces/course-demos/Rick_and_Morty_QA/resolve/main/rick.png\" width=200px>\n",
        "\"\"\"\n",
        "\n",
        "article = \"Check out [the original Rick and Morty Bot](https://huggingface.co/spaces/kingabzpro/Rick_and_Morty_Bot) that this demo is based off of.\"\n",
        "\n",
        "gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=\"textbox\",\n",
        "    outputs=\"text\",\n",
        "    title=title,\n",
        "    description=description,\n",
        "    article=article,\n",
        "    examples=[[\"What are you doing?\"], [\"Where should we time travel to?\"]],\n",
        ").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqPfWqFj-X_r"
      },
      "source": [
        "## 创建临时链接\n",
        "share=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "HvdgNec5-Lfi",
        "outputId": "847faf5a-9d7d-4cec-b7b1-c034d29bca17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8fd25c15e2924bd993.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://8fd25c15e2924bd993.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=\"textbox\",\n",
        "    outputs=\"text\",\n",
        "    title=title,\n",
        "    description=description,\n",
        "    article=article,\n",
        "    examples=[[\"What are you doing?\"], [\"Where should we time travel to?\"]],\n",
        ").launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hugging Face Spaces上托管演示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import gradio as gr\n",
        "from torch import nn\n",
        "\n",
        "LABELS = Path(\"./data/class_names.txt\").read_text().splitlines()\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(32, 64, 3, padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(64, 128, 3, padding=\"same\"),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1152, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, len(LABELS)),\n",
        ")\n",
        "state_dict = torch.load(\"./model/pytorch_model.bin\", map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def predict(im):\n",
        "    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
        "    values, indices = torch.topk(probabilities, 5)\n",
        "    return {LABELS[i]: v.item() for i, v in zip(indices, values)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py:530: UserWarning: Cannot load huggingface. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/huggingface (Request ID: Root=1-65c353b9-3a3e38df50b2a07216b793e4;cefc748b-5cc0-4860-8c86-1e8d0e7b4060)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://143931c74cd37d7f23.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://143931c74cd37d7f23.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/route_utils.py\", line 230, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/var/folders/5y/pzgqqgv92qd3p3t4m01dhbp00000gn/T/ipykernel_35605/1137200294.py\", line 29, in predict\n",
            "    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
            "TypeError: must be real number, not dict\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/route_utils.py\", line 230, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/var/folders/5y/pzgqqgv92qd3p3t4m01dhbp00000gn/T/ipykernel_35605/1137200294.py\", line 29, in predict\n",
            "    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
            "TypeError: must be real number, not dict\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/route_utils.py\", line 230, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/var/folders/5y/pzgqqgv92qd3p3t4m01dhbp00000gn/T/ipykernel_35605/1137200294.py\", line 29, in predict\n",
            "    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
            "TypeError: must be real number, not dict\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/route_utils.py\", line 230, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py\", line 1590, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/Users/huangxinzhe/opt/anaconda3/envs/huggingface10/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/var/folders/5y/pzgqqgv92qd3p3t4m01dhbp00000gn/T/ipykernel_35605/1137200294.py\", line 29, in predict\n",
            "    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
            "TypeError: must be real number, not dict\n"
          ]
        }
      ],
      "source": [
        "interface = gr.Interface(\n",
        "    predict,\n",
        "    inputs=\"sketchpad\",\n",
        "    outputs=\"label\",\n",
        "    theme=\"huggingface\",\n",
        "    title=\"Sketch Recognition\",\n",
        "    description=\"Who wants to play Pictionary? Draw a common object like a shovel or a laptop, and the algorithm will guess in real time!\",\n",
        "    article=\"<p style='text-align: center'>Sketch Recognition | Demo Model</p>\",\n",
        "    live=True,\n",
        ")\n",
        "interface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def chat(message, history):\n",
        "    history = history or []\n",
        "    if message.startswith(\"How many\"):\n",
        "        response = random.randint(1, 10)\n",
        "    elif message.startswith(\"How\"):\n",
        "        response = random.choice([\"Great\", \"Good\", \"Okay\", \"Bad\"])\n",
        "    elif message.startswith(\"Where\"):\n",
        "        response = random.choice([\"Here\", \"There\", \"Somewhere\"])\n",
        "    else:\n",
        "        response = \"I don't know\"\n",
        "    history.append((message, response))\n",
        "    return history, history\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    chat,\n",
        "    [\"text\", \"state\"],\n",
        "    [\"chatbot\", \"state\"],\n",
        "    allow_screenshot=False,\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
