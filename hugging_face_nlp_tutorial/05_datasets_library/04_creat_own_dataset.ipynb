{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建自己的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/6641',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/6641/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/6641/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/6641/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/issues/6641',\n",
       "  'id': 2116963132,\n",
       "  'node_id': 'I_kwDODunzps5-Lks8',\n",
       "  'number': 6641,\n",
       "  'title': \"unicodedecodeerror: 'utf-8' codec can't decode byte 0xac in position 25: invalid start byte\",\n",
       "  'user': {'login': 'Hughhuh',\n",
       "   'id': 109789057,\n",
       "   'node_id': 'U_kgDOBos_gQ',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/109789057?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/Hughhuh',\n",
       "   'html_url': 'https://github.com/Hughhuh',\n",
       "   'followers_url': 'https://api.github.com/users/Hughhuh/followers',\n",
       "   'following_url': 'https://api.github.com/users/Hughhuh/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/Hughhuh/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/Hughhuh/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/Hughhuh/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/Hughhuh/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/Hughhuh/repos',\n",
       "   'events_url': 'https://api.github.com/users/Hughhuh/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/Hughhuh/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 0,\n",
       "  'created_at': '2024-02-04T08:49:31Z',\n",
       "  'updated_at': '2024-02-04T08:49:31Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'NONE',\n",
       "  'active_lock_reason': None,\n",
       "  'body': '### Describe the bug\\n\\nunicodedecodeerror: \\'utf-8\\' codec can\\'t decode byte 0xac in position 25: invalid start byte\\n\\n### Steps to reproduce the bug\\n\\nimport sys\\r\\nsys.getdefaultencoding()\\r\\n\\'utf-8\\'\\r\\n\\r\\n\\r\\n\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nprint(f\"Train dataset size: {len(dataset[\\'train\\'])}\")\\r\\nprint(f\"Test dataset size: {len(dataset[\\'test\\'])}\")\\r\\n\\r\\nResolving data files: 100%\\r\\n159/159 [00:00<00:00, 9909.28it/s]\\r\\nUsing custom data configuration samsum-0b1209637541c9e6\\r\\nDownloading and preparing dataset json/samsum to C:/Users/Administrator/.cache/huggingface/datasets/json/samsum-0b1209637541c9e6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\\r\\nDownloading data files: 100%\\r\\n3/3 [00:00<00:00, 119.99it/s]\\r\\nExtracting data files: 100%\\r\\n3/3 [00:00<00:00, 9.54it/s]\\r\\nGenerating train split:\\r\\n88392/0 [00:15<00:00, 86848.17 examples/s]\\r\\nGenerating test split:\\r\\n0/0 [00:00<?, ? examples/s]\\r\\n---------------------------------------------------------------------------\\r\\nArrowInvalid                              Traceback (most recent call last)\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\datasets\\\\packaged_modules\\\\json\\\\json.py:132, in Json._generate_tables(self, files)\\r\\n    131 try:\\r\\n--> 132     pa_table = paj.read_json(\\r\\n    133         io.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)\\r\\n    134     )\\r\\n    135     break\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\pyarrow\\\\_json.pyx:290, in pyarrow._json.read_json()\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\pyarrow\\\\error.pxi:144, in pyarrow.lib.pyarrow_internal_check_status()\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\pyarrow\\\\error.pxi:100, in pyarrow.lib.check_status()\\r\\n\\r\\nArrowInvalid: JSON parse error: Invalid value. in row 0\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nUnicodeDecodeError                        Traceback (most recent call last)\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\datasets\\\\builder.py:1819, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\\r\\n   1818 _time = time.time()\\r\\n-> 1819 for _, table in generator:\\r\\n   1820     if max_shard_size is not None and writer._num_bytes > max_shard_size:\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\datasets\\\\packaged_modules\\\\json\\\\json.py:153, in Json._generate_tables(self, files)\\r\\n    152     with open(file, encoding=\"utf-8\") as f:\\r\\n--> 153         dataset = json.load(f)\\r\\n    154 except json.JSONDecodeError:\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\json\\\\__init__.py:293, in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\\r\\n    276 \"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\\r\\n    277 a JSON document) to a Python object.\\r\\n    278 \\r\\n   (...)\\r\\n    291 kwarg; otherwise ``JSONDecoder`` is used.\\r\\n    292 \"\"\"\\r\\n--> 293 return loads(fp.read(),\\r\\n    294     cls=cls, object_hook=object_hook,\\r\\n    295     parse_float=parse_float, parse_int=parse_int,\\r\\n    296     parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\\r\\n    321 data = self.buffer + input\\r\\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\\r\\n    323 # keep undecoded input until the next call\\r\\n\\r\\nUnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xac in position 25: invalid start byte\\r\\n\\r\\nThe above exception was the direct cause of the following exception:\\r\\n\\r\\nDatasetGenerationError                    Traceback (most recent call last)\\r\\nCell In[81], line 5\\r\\n      1 from datasets import load_dataset\\r\\n      3 # Load dataset from the hub\\r\\n      4 #dataset = load_dataset(\"json\",data_files=\"C:/Users/Administrator/Desktop/samsum/samsum/data/corpus/train.json\",field=\"data\")\\r\\n----> 5 dataset = load_dataset(\\'json\\',\"samsum\")\\r\\n      6 #dataset = load_dataset(\"samsum\")\\r\\n      7 print(f\"Train dataset size: {len(dataset[\\'train\\'])}\")\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\datasets\\\\load.py:1758, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\\r\\n   1755 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\\r\\n   1757 # Download and prepare data\\r\\n-> 1758 builder_instance.download_and_prepare(\\r\\n   1759     download_config=download_config,\\r\\n   1760     download_mode=download_mode,\\r\\n   1761     ignore_verifications=ignore_verifications,\\r\\n   1762     try_from_hf_gcs=try_from_hf_gcs,\\r\\n   1763     num_proc=num_proc,\\r\\n   1764 )\\r\\n   1766 # Build dataset for splits\\r\\n   1767 keep_in_memory = (\\r\\n   1768     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\\r\\n   1769 )\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\datasets\\\\builder.py:860, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\\r\\n    858     if num_proc is not None:\\r\\n    859         prepare_split_kwargs[\"num_proc\"] = num_proc\\r\\n--> 860     self._download_and_prepare(\\r\\n    861         dl_manager=dl_manager,\\r\\n    862         verify_infos=verify_infos,\\r\\n    863         **prepare_split_kwargs,\\r\\n    864         **download_and_prepare_kwargs,\\r\\n    865     )\\r\\n    866 # Sync info\\r\\n    867 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\datasets\\\\builder.py:953, in DatasetBuilder._download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\\r\\n    949 split_dict.add(split_generator.split_info)\\r\\n    951 try:\\r\\n    952     # Prepare split will record examples associated to the split\\r\\n--> 953     self._prepare_split(split_generator, **prepare_split_kwargs)\\r\\n    954 except OSError as e:\\r\\n    955     raise OSError(\\r\\n    956         \"Cannot find data file. \"\\r\\n    957         + (self.manual_download_instructions or \"\")\\r\\n    958         + \"\\\\nOriginal error:\\\\n\"\\r\\n    959         + str(e)\\r\\n    960     ) from None\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\datasets\\\\builder.py:1708, in ArrowBasedBuilder._prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\\r\\n   1706 gen_kwargs = split_generator.gen_kwargs\\r\\n   1707 job_id = 0\\r\\n-> 1708 for job_id, done, content in self._prepare_split_single(\\r\\n   1709     gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\\r\\n   1710 ):\\r\\n   1711     if done:\\r\\n   1712         result = content\\r\\n\\r\\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\datasets\\\\builder.py:1851, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\\r\\n   1849     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\\r\\n   1850         e = e.__context__\\r\\n-> 1851     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\\r\\n   1853 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\\r\\n\\r\\nDatasetGenerationError: An error occurred while generating the dataset\\r\\n\\n\\n### Expected behavior\\n\\ncan\\'t load dataset\\n\\n### Environment info\\n\\ndataset:samsum\\r\\nsystem :win10\\r\\ngpu:m40 24G',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/6641/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/6641/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")  # Copy your GitHub token here\n",
    "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(\n",
    "            f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\",\n",
    "               orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e6fd7323a74d12910a8c25f2176b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached GitHub rate limit. Sleeping for one hour ...\n",
      "Downloaded all the issues for datasets! Dataset stored at ./datasets-issues.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Depending on your internet connection, this can take several minutes to run...\n",
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "issues_dataset = load_dataset(\n",
    "    \"json\", data_files=\"./data/issues.jsonl\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、清理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return [r[\"body\"] for r in response.json()]\n",
    "\n",
    "\n",
    "# Test our function works as expected\n",
    "get_comments(2792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your internet connection, this can take a few minutes...\n",
    "issues_with_comments_dataset = issues_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、扩充数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_number = 2792\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、上传huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "登录huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "# 命令行输入\n",
    "# huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上传数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_with_comments_dataset.push_to_hub(\"github-issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "remote_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据集卡片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建信息性数据集卡片的指南\n",
    "https://github.com/huggingface/datasets/blob/main/templates/README_guide.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
